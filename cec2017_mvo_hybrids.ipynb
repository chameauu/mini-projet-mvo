{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c50413f",
   "metadata": {},
   "source": [
    "# CEC2017 Benchmark: MVO vs Hybrids\n",
    "\n",
    "Compare MVO, Sequential Hybrid, and Parallel Hybrid on CEC2017 functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee06e59",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b649cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'algorithms'))\n",
    "\n",
    "from cec2017.functions import all_functions\n",
    "from mvo import MVO\n",
    "from pso import PSO\n",
    "from mgwo import MGWO\n",
    "from sequential_hybrid import sequential_hybrid\n",
    "from parallel_hybrid import parallel_hybrid\n",
    "from ga import GA\n",
    "from gsa import GSA\n",
    "from abc import ABC\n",
    "\n",
    "# Create results folder\n",
    "RESULTS_FOLDER = 'results/cec2017_mvo_hybrids'\n",
    "os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f'CEC2017 functions loaded: {len(all_functions)}')\n",
    "print(f'Results will be saved to: {RESULTS_FOLDER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf164e",
   "metadata": {},
   "source": [
    "## CEC2017 Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e26d6d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def cec2017_wrapper(func, name):\n",
    "    \"\"\"Wrapper to handle CEC2017 function input format (expects 2D array)\"\"\"\n",
    "    def wrapped(x):\n",
    "        x_2d = np.array(x).reshape(1, -1)\n",
    "        return func(x_2d)[0]\n",
    "    wrapped.__name__ = name\n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c445b6",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 10           # CEC2017 supports: 10, 30, 50, 100\n",
    "POP_SIZE = 30\n",
    "MAX_ITER = 1000\n",
    "NUM_RUNS = 1       # Increase for statistical analysis\n",
    "LB = -100          # CEC2017 bounds\n",
    "UB = 100\n",
    "\n",
    "print(f'Dimensions: {DIM}')\n",
    "print(f'Population: {POP_SIZE}')\n",
    "print(f'Iterations: {MAX_ITER}')\n",
    "print(f'Runs per function: {NUM_RUNS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837354e9",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_FOLDER = f'{RESULTS_FOLDER}/convergence'\n",
    "os.makedirs(CONV_FOLDER, exist_ok=True)\n",
    "\n",
    "colors = {\n",
    "    'MVO': '#FF0000',        # Bright Red\n",
    "    'PSO': '#0000FF',        # Bright Blue\n",
    "    'MGWO': '#00FF00',       # Bright Green\n",
    "    'Sequential': '#FF00FF',  # Magenta\n",
    "    'Parallel': '#00FFFF',    # Cyan\n",
    "    'GA': '#FF8800',         # Orange\n",
    "    'GSA': '#8800FF',        # Purple\n",
    "    'ABC': '#FFD700'         # Gold\n",
    "}\n",
    "algos_list = ['MVO', 'PSO', 'MGWO', 'Sequential', 'Parallel', 'GA', 'GSA', 'ABC']\n",
    "\n",
    "# CSV path for incremental saving\n",
    "results_csv_path = f'{RESULTS_FOLDER}/results_incremental.csv'\n",
    "\n",
    "solutions_csv_path = f'{RESULTS_FOLDER}/best_solutions.csv'\n",
    "\n",
    "def save_function_result(func_id, func_results, func_best_solutions):\n",
    "    \"\"\"Save single function result and best solutions to CSV immediately after execution\"\"\"\n",
    "    valid_results = {k: v for k, v in func_results.items() if not np.isinf(v)}\n",
    "    winner = min(valid_results, key=valid_results.get) if valid_results else 'N/A'\n",
    "    \n",
    "    # Save fitness results\n",
    "    row = {\n",
    "        'Function': f'F{func_id:02d}',\n",
    "        'MVO': f\"{func_results['MVO']:.6e}\",\n",
    "        'PSO': f\"{func_results['PSO']:.6e}\",\n",
    "        'MGWO': f\"{func_results['MGWO']:.6e}\",\n",
    "        'Sequential': f\"{func_results['Sequential']:.6e}\",\n",
    "        'Parallel': f\"{func_results['Parallel']:.6e}\",\n",
    "        'GA': f\"{func_results['GA']:.6e}\",\n",
    "        'GSA': f\"{func_results['GSA']:.6e}\",\n",
    "        'ABC': f\"{func_results['ABC']:.6e}\",\n",
    "        'Winner': winner\n",
    "    }\n",
    "    \n",
    "    file_exists = os.path.exists(results_csv_path)\n",
    "    df_row = pd.DataFrame([row])\n",
    "    df_row.to_csv(results_csv_path, mode='a', header=not file_exists, index=False)\n",
    "    \n",
    "    # Save best solutions (points)\n",
    "    for algo in algos_list:\n",
    "        if func_best_solutions[algo] is not None:\n",
    "            sol_row = {\n",
    "                'Function': f'F{func_id:02d}',\n",
    "                'Algorithm': algo,\n",
    "                'Fitness': f\"{func_results[algo]:.6e}\",\n",
    "                'Solution': ','.join([f\"{x:.6f}\" for x in func_best_solutions[algo]])\n",
    "            }\n",
    "            sol_exists = os.path.exists(solutions_csv_path)\n",
    "            df_sol = pd.DataFrame([sol_row])\n",
    "            df_sol.to_csv(solutions_csv_path, mode='a', header=not sol_exists, index=False)\n",
    "    \n",
    "    print(f\"✓ Results saved\")\n",
    "\n",
    "\n",
    "def run_single_function(func_id):\n",
    "    \"\"\"Run all algorithms on a single function and plot convergence\"\"\"\n",
    "    \n",
    "    raw_func = all_functions[func_id - 1]\n",
    "    func = cec2017_wrapper(raw_func, f\"F{func_id:02d}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Function F{func_id:02d}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    func_results = {}\n",
    "    func_convergence = {}\n",
    "    \n",
    "    func_best_solutions = {}\n",
    "    \n",
    "    # Run each algorithm\n",
    "    for algo_name in algos_list:\n",
    "        print(f\"Running {algo_name}...\", end=\" \", flush=True)\n",
    "        \n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            try:\n",
    "                if algo_name == 'MVO':\n",
    "                    result = MVO(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'PSO':\n",
    "                    result = PSO(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'MGWO':\n",
    "                    result = MGWO(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'Sequential':\n",
    "                    result = sequential_hybrid(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'Parallel':\n",
    "                    result = parallel_hybrid(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'GA':\n",
    "                    result = GA(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'GSA':\n",
    "                    result = GSA(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                elif algo_name == 'ABC':\n",
    "                    result = ABC(func, LB, UB, DIM, POP_SIZE, MAX_ITER)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown algorithm: {algo_name}\")\n",
    "                \n",
    "                # Get the final best fitness (last value in convergence)\n",
    "                conv = result.convergence\n",
    "                fitness = conv[-1]\n",
    "                func_results[algo_name] = fitness\n",
    "                func_convergence[algo_name] = conv\n",
    "                func_best_solutions[algo_name] = result.bestIndividual\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                func_results[algo_name] = float('inf')\n",
    "                func_convergence[algo_name] = np.full(MAX_ITER, float('inf'))\n",
    "                func_best_solutions[algo_name] = None\n",
    "        \n",
    "        print(f\"Done! Fitness: {func_results[algo_name]:.4e}\")\n",
    "    \n",
    "    # Find winner\n",
    "    winner = min(func_results, key=func_results.get)\n",
    "    print(f\"\\n*** Winner: {winner} (Fitness: {func_results[winner]:.4e}) ***\")\n",
    "    \n",
    "    # Plot convergence curve (log scale only)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    iterations = np.arange(1, MAX_ITER + 1)\n",
    "    \n",
    "    for algo in algos_list:\n",
    "        ax.semilogy(iterations, func_convergence[algo], \n",
    "                    label=f\"{algo} ({func_results[algo]:.2e})\", \n",
    "                    color=colors[algo], linewidth=2.0, alpha=0.85)\n",
    "    ax.set_xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Fitness (log)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'CEC2017 F{func_id:02d} - Convergence | Winner: {winner}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(alpha=0.3, which='both')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    save_path = f'{CONV_FOLDER}/F{func_id:02d}_convergence.png'\n",
    "    plt.savefig(save_path, dpi=200, bbox_inches='tight')\n",
    "    print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return func_results, func_convergence, func_best_solutions\n",
    "\n",
    "# Run benchmark on all 30 functions\n",
    "results = {}\n",
    "convergence = {}\n",
    "best_solutions = {}\n",
    "\n",
    "for func_id in range(1, 31):\n",
    "    func_results, func_convergence, func_best_solutions = run_single_function(func_id)\n",
    "    results[func_id] = {algo: [func_results[algo]] for algo in algos_list}\n",
    "    convergence[func_id] = func_convergence\n",
    "    best_solutions[func_id] = func_best_solutions\n",
    "    \n",
    "    # Save immediately after each function\n",
    "    save_function_result(func_id, func_results, func_best_solutions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e1637",
   "metadata": {},
   "source": [
    "## Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(results):\n",
    "    \"\"\"Create and display results table\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    wins = {algo: 0 for algo in algos_list}\n",
    "    \n",
    "    # Only process functions that were actually run\n",
    "    for func_id in sorted(results.keys()):\n",
    "        means = {algo: np.mean(results[func_id][algo]) for algo in algos_list}\n",
    "        winner = min(means, key=means.get)\n",
    "        wins[winner] += 1\n",
    "        \n",
    "        row_data = {'Function': f'F{func_id:02d}'}\n",
    "        for algo in algos_list:\n",
    "            row_data[algo] = means[algo]\n",
    "        row_data['Winner'] = winner\n",
    "        data.append(row_data)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    csv_path = f'{RESULTS_FOLDER}/final_results.csv'\n",
    "    df.to_csv(csv_path, index=False, float_format='%.6e')\n",
    "    print(f\"Results saved to: {csv_path}\")\n",
    "    \n",
    "    return df, wins\n",
    "\n",
    "df_results, wins = create_results_table(results)\n",
    "print(\"\\nResults Table:\")\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4f8d0",
   "metadata": {},
   "source": [
    "## Win Count Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04268a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WIN COUNT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for algo, count in sorted(wins.items(), key=lambda x: x[1], reverse=True):\n",
    "    bar = \"█\" * count\n",
    "    print(f\"{algo:15s}: {count:2d} wins {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082f8a8",
   "metadata": {},
   "source": [
    "## Visualization: Win Count Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7304b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "bar_colors = [colors[a] for a in algos_list]\n",
    "\n",
    "bars = ax.bar(algos_list, [wins[a] for a in algos_list], color=bar_colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Number of Wins', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'CEC2017: Algorithm Comparison Win Count\\n({DIM}D, {MAX_ITER} iterations)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "max_wins = max(wins.values()) if wins.values() else 0\n",
    "ax.set_ylim(0, max_wins + 2)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "for bar, algo in zip(bars, algos_list):\n",
    "    count = wins[algo]\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "            str(count), ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_FOLDER}/win_count.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {RESULTS_FOLDER}/win_count.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20060baf",
   "metadata": {},
   "source": [
    "## Visualization: Performance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0934947",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "perf_matrix = df_results[algos_list].values\n",
    "\n",
    "normalized = np.zeros_like(perf_matrix)\n",
    "for i in range(perf_matrix.shape[0]):\n",
    "    row_min = perf_matrix[i].min()\n",
    "    row_max = perf_matrix[i].max()\n",
    "    if row_max > row_min:\n",
    "        normalized[i] = (perf_matrix[i] - row_min) / (row_max - row_min)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "\n",
    "im = ax.imshow(normalized, cmap='RdYlGn_r', aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(len(algos_list)))\n",
    "ax.set_xticklabels(algos_list, fontsize=10, rotation=45, ha='right')\n",
    "num_funcs = len(df_results)\n",
    "ax.set_yticks(range(num_funcs))\n",
    "ax.set_yticklabels(df_results['Function'].tolist(), fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Function', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Normalized Performance\\n(Green=Best, Red=Worst)', fontsize=14, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label('Normalized Performance (0=Best, 1=Worst)', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_FOLDER}/performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {RESULTS_FOLDER}/performance_heatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb6a11",
   "metadata": {},
   "source": [
    "## Visualization: Category Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'Unimodal (F1-F3)': range(1, 4),\n",
    "    'Multimodal (F4-F10)': range(4, 11),\n",
    "    'Hybrid (F11-F20)': range(11, 21),\n",
    "    'Composition (F21-F30)': range(21, 31)\n",
    "}\n",
    "\n",
    "# Get list of completed function IDs\n",
    "completed_funcs = set(results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (cat_name, func_range) in enumerate(categories.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    cat_wins = {algo: 0 for algo in algos_list}\n",
    "    funcs_in_cat = [f for f in func_range if f in completed_funcs]\n",
    "    \n",
    "    for func_id in funcs_in_cat:\n",
    "        row = df_results[df_results['Function'] == f'F{func_id:02d}']\n",
    "        if len(row) > 0:\n",
    "            winner = row['Winner'].values[0]\n",
    "            cat_wins[winner] += 1\n",
    "    \n",
    "    counts_list = [cat_wins[a] for a in algos_list]\n",
    "    bar_colors_cat = [colors[a] for a in algos_list]\n",
    "    \n",
    "    bars = ax.bar(algos_list, counts_list, color=bar_colors_cat, alpha=0.8, edgecolor='black')\n",
    "    ax.set_title(f\"{cat_name} ({len(funcs_in_cat)} funcs)\", fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Wins')\n",
    "    max_count = max(counts_list) if counts_list else 0\n",
    "    ax.set_ylim(0, max(len(funcs_in_cat) if funcs_in_cat else 1, max_count + 1))\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "    \n",
    "    for bar, algo in zip(bars, algos_list):\n",
    "        count = cat_wins[algo]\n",
    "        if count > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    str(count), ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Performance by Function Category', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_FOLDER}/category_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {RESULTS_FOLDER}/category_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742190d8",
   "metadata": {},
   "source": [
    "## Visualization: Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae83623",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "algo_data = {algo: [] for algo in algos_list}\n",
    "for func_id in results.keys():\n",
    "    for algo in algo_data.keys():\n",
    "        algo_data[algo].extend(results[func_id][algo])\n",
    "\n",
    "log_data = []\n",
    "labels = []\n",
    "for algo in algos_list:\n",
    "    vals = np.array(algo_data[algo])\n",
    "    vals = np.clip(vals, 1e-10, None)\n",
    "    log_data.append(np.log10(vals))\n",
    "    labels.append(algo)\n",
    "\n",
    "bp = ax.boxplot(log_data, labels=labels, patch_artist=True)\n",
    "\n",
    "colors_list = [colors[a] for a in algos_list]\n",
    "for patch, color in zip(bp['boxes'], colors_list):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('log10(Fitness)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Fitness Values', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_FOLDER}/fitness_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {RESULTS_FOLDER}/fitness_boxplot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c65ed",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Dimensions: {DIM}\")\n",
    "print(f\"  Population: {POP_SIZE}\")\n",
    "print(f\"  Iterations: {MAX_ITER}\")\n",
    "\n",
    "num_completed = len(results)\n",
    "print(f\"\\nOverall Win Count ({num_completed} functions):\")\n",
    "for algo, count in sorted(wins.items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = count / num_completed * 100 if num_completed > 0 else 0\n",
    "    print(f\"  {algo:15s}: {count:2d}/{num_completed} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {RESULTS_FOLDER}/\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
